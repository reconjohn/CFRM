\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={CFRM 521 Final Project},
            pdfauthor={Yohan Min \& Peiyuan Song},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{CFRM 521 Final Project}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Yohan Min \& Peiyuan Song}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{June 4, 2018}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\usepackage{float}
\floatplacement{figure}{H}
\usepackage[fontsize=12pt]{scrextend}

\begin{document}
\maketitle

\section{Predicting Stock Price Movement Using Social Media
Analysis}\label{predicting-stock-price-movement-using-social-media-analysis}

\subsection{Abstract}\label{abstract}

The purpose of this final project is to understand the research and
replicate methods used in Derik Tsui's paper. We tried the 3 methods,
Naive bayes, Support vector regression and K-nearest neighbors
regression to compare with the original analysis. It turns out the
results are slightly different due to the fact that the data for these
analyses may be different we assume.

\subsection{Background}\label{background}

Today's social media platforms is one of the most fast and efficient way
that transport information among people as well as the financial market.
This brings us an idea of study the relationship between people's
comments on social media platforms and same period financial market
performance. Some platforms like StockTwits provide a good and detailed
resource of comments database, which we can build our model directly
based on it.

Because of more and more financial institutions now start to adapt
automated statistical techniques in their daily trading practices, the
sample paper believes that further development of large-scale social
media analysis will have the potential to introduce an additional source
of investment alpha.

Same as our sample paper, our underlying assumption is that a
correlation between aggregated sentiment indicator and the market price
reaction. Thus, our StockTwits data which represent market sentiment can
provide a robust and meaningful information of real financial market
situation.

\subsection{Method}\label{method}

\subsubsection{Data}\label{data}

We first download data from StockTwits, the data is only available in
JSON format, and contains more than 566,000 comments data, covering 1592
stocks from the beginning of 2013 all the way through the end of 2016.
Each data point is comprised of message body, timestamp, sentiment, and
ticker symbols.

We will first need to convert our raw data into txt or csv format for
the ease of process with R. By doing so, we will use jsonlite package
and convert raw data into matrix format. During this process, we will
remove all message body, and only leave useful information in our
processed sentiment dataset.

Price data for DJI Average can be retrieved using quantmod package,
where we will get four years of pricing data from 2013 - 2016.
Calculating forward 3-day return using exactly same method described by
the sample paper. Reason of using 3-day return is to smooth out
short-term volatility and market noise.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{strt =}\StringTok{ }\KeywordTok{read.table}\NormalTok{(}\StringTok{"../data/strt.txt"}\NormalTok{,}\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{strt =}\StringTok{ }\NormalTok{strt[,}\OperatorTok{-}\DecValTok{1}\OperatorTok{:-}\DecValTok{2}\NormalTok{]}
\NormalTok{strt[,}\DecValTok{1}\NormalTok{] =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(strt[,}\DecValTok{1}\NormalTok{])}
\CommentTok{#strt = ifelse(strt==0, "n", "y") }
\NormalTok{dat_train =}\StringTok{ }\NormalTok{strt[}\OperatorTok{-}\DecValTok{757}\OperatorTok{:-}\DecValTok{1005}\NormalTok{,]}
\NormalTok{dat_test =}\StringTok{ }\NormalTok{strt[}\DecValTok{757}\OperatorTok{:}\DecValTok{1005}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

Instead of using bag of words described in the sample paper, we will use
number of bullish and bearish message in a single day. For example, if
in a single day, bullish comments toward DJI's constituent stocks is
more than that of bearish comments, we will identify that day as a day
with bullish sentiment, and vice versa. That is, sentiment parameter =
1, when (\# of bullish message) \textgreater{} (\# of bearish message).

\rowcolors{2}{gray!6}{white}

\begin{table}[H]

\caption{\label{tab:unnamed-chunk-2}Cleaned data table (first 15 rows)}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrrrrrrrrrrrrrrrrrrrrrrr}
\hiderowcolors
\toprule
DJI.Close & MMM & AXP & AAPL & BA & CAT & CVX & CSCO & KO & DIS & XOM & GE & GS & HD & IBM & INTC & JNJ & JPM & MCD & MRK & MSFT & NKE & PFE & PG & TRV & UTX & UNH & VZ & V & WMT\\
\midrule
\showrowcolors
0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\\
0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
0 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0\\
\addlinespace
1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 1\\
1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
1 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1\\
1 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
1 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1\\
\addlinespace
1 & 0 & 0 & 1 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\
1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1\\
1 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1\\
1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0\\
\bottomrule
\end{tabular}}
\end{table}

\rowcolors{2}{white}{white}

\subsubsection{Training}\label{training}

Next, we will try methods described in the sample paper and try to
replicate similar results. Similar to the method as described in sample
paper, we will avoid look ahead bias by set 1st 75\% of historical data
as training data, and the remaining 25\% of data as the test set (data
from Jan 2016 - Dec 2016). Trainings are all performed in dinfferent
methods: NB, SVR and KNN. The parameters after each train are applied to
test data to compare with the real value.

\newpage

\subsection{Results}\label{results}

\subsubsection{Naive Bayes Analysis}\label{naive-bayes-analysis}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(naivebayes)}
\CommentTok{# training}
\NormalTok{nb =}\StringTok{ }\KeywordTok{naive_bayes}\NormalTok{(DJI.Close}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\KeywordTok{as.data.frame}\NormalTok{(dat_train))}
\CommentTok{#tables(nb)}
\CommentTok{#summary(nb)}

\CommentTok{# predicting}
\CommentTok{#dat_test}
\CommentTok{#predict(nb, dat_test[,-1], type = "class") }
\CommentTok{#predict(nb, dat_test[,-1], type = "prob")}
\NormalTok{prd =}\StringTok{ }\KeywordTok{predict}\NormalTok{(nb, dat_test[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], }\DataTypeTok{type =} \StringTok{"class"}\NormalTok{) }
\CommentTok{#dat_test[,1]==prd}
\CommentTok{#sum(dat_test[,1]!=prd) # number of difference}
\NormalTok{nbv =}\StringTok{ }\KeywordTok{sum}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{]}\OperatorTok{==}\NormalTok{prd)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{]) }\CommentTok{# % of correct}
\end{Highlighting}
\end{Shaded}

\rowcolors{2}{gray!6}{white} \rowcolors{2}{gray!6}{white}

\begin{table}[H]
\caption{\label{tab:unnamed-chunk-3}NB train results in order of MMM,AXP,AAPL,BA,CAT,CVX,CSCO,KO,
DIS,XOM,GE,GS,HD,IBM,INTC,JNJ,JPM,MCD,MRK,MSFT,NKE,PFE,PG,TRV,UTX,UNH,VZ,V,
WMT}

\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrr}
\hiderowcolors
\hiderowcolors
\toprule
  & 0 & 1\\
\midrule
\showrowcolors
\showrowcolors
mean & 0.1550633 & 0.1750000\\
sd & 0.3625391 & 0.3803996\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4303797 & 0.4045455\\
sd & 0.4959146 & 0.4913625\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.9588608 & 0.9386364\\
sd & 0.1989272 & 0.2402693\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.6455696 & 0.6636364\\
sd & 0.4790990 & 0.4730028\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.3291139 & 0.375000\\
sd & 0.4706367 & 0.484674\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4430380 & 0.4454545\\
sd & 0.4975326 & 0.4975816\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.6708861 & 0.6181818\\
sd & 0.4706367 & 0.4863854\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4620253 & 0.4863636\\
sd & 0.4993466 & 0.5003830\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.7879747 & 0.7840909\\
sd & 0.4093910 & 0.4119199\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4810127 & 0.4340909\\
sd & 0.5004318 & 0.4962011\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.6234177 & 0.5431818\\
sd & 0.4852972 & 0.4986989\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.5696203 & 0.5818182\\
sd & 0.4959146 & 0.4938218\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4683544 & 0.4340909\\
sd & 0.4997890 & 0.4962011\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.5063291 & 0.5545455\\
sd & 0.5007529 & 0.4975816\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.6424051 & 0.6431818\\
sd & 0.4800522 & 0.4796058\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4462025 & 0.4022727\\
sd & 0.4978858 & 0.4909146\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4493671 & 0.375000\\
sd & 0.4982186 & 0.484674\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.3607595 & 0.2772727\\
sd & 0.4809825 & 0.4481618\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.3449367 & 0.3045455\\
sd & 0.4761016 & 0.4607385\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.8639241 & 0.8250000\\
sd & 0.3434130 & 0.3803996\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.6075949 & 0.5590909\\
sd & 0.4890606 & 0.4970611\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4778481 & 0.4840909\\
sd & 0.5003013 & 0.5003157\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.2943038 & 0.2568182\\
sd & 0.4564520 & 0.4373755\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.0949367 & 0.0727273\\
sd & 0.2935924 & 0.2599839\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.1867089 & 0.2068182\\
sd & 0.3902957 & 0.4054850\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.306962 & 0.2363636\\
sd & 0.461965 & 0.4253317\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4525316 & 0.4909091\\
sd & 0.4985311 & 0.5004864\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.6740506 & 0.6636364\\
sd & 0.4694719 & 0.4730028\\
\bottomrule
\end{tabular}
\centering
\begin{tabular}[t]{lrr}
\toprule
  & 0 & 1\\
\midrule
mean & 0.4683544 & 0.4795455\\
sd & 0.4997890 & 0.5001501\\
\bottomrule
\end{tabular}
\end{table}

\rowcolors{2}{white}{white} \rowcolors{2}{white}{white}

\newpage

\pagebreak

\subsubsection{Support Vector Regression
Analysis}\label{support-vector-regression-analysis}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(e1071)}
\NormalTok{modelsvm =}\StringTok{ }\KeywordTok{svm}\NormalTok{(DJI.Close}\OperatorTok{~}\NormalTok{., dat_train)}
\NormalTok{predYsvm =}\StringTok{ }\KeywordTok{predict}\NormalTok{(modelsvm, dat_test[,}\OperatorTok{-}\DecValTok{1}\NormalTok{])}
\NormalTok{svrv =}\StringTok{ }\KeywordTok{sum}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{] }\OperatorTok{==}\StringTok{ }\NormalTok{predYsvm)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{])}
\KeywordTok{summary}\NormalTok{(modelsvm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## svm(formula = DJI.Close ~ ., data = dat_train)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
##       gamma:  0.03448276 
## 
## Number of Support Vectors:  706
## 
##  ( 316 390 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  0 1
\end{verbatim}

\pagebreak

\subsubsection{K-Nearest Neighbors Regression
Analysis}\label{k-nearest-neighbors-regression-analysis}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(class)}
\NormalTok{knn.}\DecValTok{1}\NormalTok{ <-}\StringTok{  }\KeywordTok{knn}\NormalTok{(dat_train[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], dat_test[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], dat_train[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{k=}\DecValTok{1}\NormalTok{)}
\NormalTok{knn.}\DecValTok{5}\NormalTok{ <-}\StringTok{  }\KeywordTok{knn}\NormalTok{(dat_train[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], dat_test[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], dat_train[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{knn.}\DecValTok{20}\NormalTok{ <-}\StringTok{ }\KeywordTok{knn}\NormalTok{(dat_train[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], dat_test[,}\OperatorTok{-}\DecValTok{1}\NormalTok{], dat_train[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{k=}\DecValTok{20}\NormalTok{)}

\NormalTok{## Let's calculate the proportion of correct classification for k = 1, 5 & 20 }
\NormalTok{knnv1 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{] }\OperatorTok{==}\StringTok{ }\NormalTok{knn.}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{]) }\CommentTok{# For knn = 1}
\NormalTok{knnv5 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{] }\OperatorTok{==}\StringTok{ }\NormalTok{knn.}\DecValTok{5}\NormalTok{)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{])}
\NormalTok{knnv20 =}\StringTok{ }\KeywordTok{sum}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{] }\OperatorTok{==}\StringTok{ }\NormalTok{knn.}\DecValTok{20}\NormalTok{)}\OperatorTok{/}\KeywordTok{length}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{])}

\CommentTok{# table(knn.1 ,dat_test[,1])}
\CommentTok{# table(knn.5 ,dat_test[,1])}
\CommentTok{# table(knn.20 ,dat_test[,1])}
\end{Highlighting}
\end{Shaded}

\rowcolors{2}{gray!6}{white}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-4}Test accuracy, compared KNN with 
      1,5, and 20}
\centering
\begin{tabular}[t]{lr}
\hiderowcolors
\toprule
  & Test Accuracy\\
\midrule
\showrowcolors
KNN1 & 0.4980\\
KNN5 & 0.4859\\
KNN20 & 0.5301\\
Average & 0.5047\\
\bottomrule
\end{tabular}
\end{table}

\rowcolors{2}{white}{white}

\newpage

\subsection{Summary \& Discussion}\label{summary-discussion}

Among the 3 methods, SVR and KNN perform better than NB with the test
accuracy of 51.81\% an 49.8\% respectively. NB has the accuracy of
49.4\%. These results are a bit different from the original anaysis that
shows the results of accuracy for NB, SVR and KNN as 50.9\%, 56.82\% and
54.48\%.

\rowcolors{2}{gray!6}{white}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-6}Test accuracy, compared}
\centering
\begin{tabular}[t]{lr}
\hiderowcolors
\toprule
  & Test Accuracy\\
\midrule
\showrowcolors
Naive.Bayes & 0.4940\\
SVR & 0.5181\\
KNN & 0.4980\\
Average & 0.5033\\
\bottomrule
\end{tabular}
\end{table}

\rowcolors{2}{white}{white}

Also with respect to the individual stock predicting the market of stock
price movement, our result shows that CAT has the highest test accuracy
as 63.5\% while AAPL has the lowest test accuracy of 30.5\%. The overall
discrepency between the original analysis and our results may be due to
the difference of data used for analyzing the stock price movement.
Since we can't figure out how the data is different and how the data the
orginal analysis is based on was processed and cleaned, we are just
guessing. But in general, we also found that NB performed less than the
other two methods (i.e.~SVR and KNN).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{e =}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\OperatorTok{:}\DecValTok{30}\NormalTok{) e[i] =}\StringTok{ }\KeywordTok{sum}\NormalTok{(prd}\OperatorTok{==}\NormalTok{dat_test[,i])}\OperatorTok{/}\KeywordTok{length}\NormalTok{(dat_test[,}\DecValTok{1}\NormalTok{])}
\NormalTok{e =}\StringTok{ }\NormalTok{e[}\OperatorTok{-}\DecValTok{1}\NormalTok{]}
\KeywordTok{names}\NormalTok{(e) =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"MMM"}\NormalTok{,}\StringTok{"AXP"}\NormalTok{,}\StringTok{"AAPL"}\NormalTok{,}\StringTok{"BA"}\NormalTok{,}\StringTok{"CAT"}\NormalTok{,}\StringTok{"CVX"}\NormalTok{,}\StringTok{"CSCO"}\NormalTok{,}\StringTok{"KO"}\NormalTok{,}\StringTok{"DIS"}\NormalTok{,}\StringTok{"XOM"}\NormalTok{,}
             \StringTok{"GE"}\NormalTok{,}\StringTok{"GS"}\NormalTok{,}\StringTok{"HD"}\NormalTok{,}\StringTok{"IBM"}\NormalTok{,}\StringTok{"INTC"}\NormalTok{,}\StringTok{"JNJ"}\NormalTok{,}\StringTok{"JPM"}\NormalTok{,}\StringTok{"MCD"}\NormalTok{,}\StringTok{"MRK"}\NormalTok{,}\StringTok{"MSFT"}\NormalTok{,}
             \StringTok{"NKE"}\NormalTok{, }\StringTok{"PFE"}\NormalTok{,}\StringTok{"PG"}\NormalTok{,}\StringTok{"TRV"}\NormalTok{,}\StringTok{"UTX"}\NormalTok{,}\StringTok{"UNH"}\NormalTok{,}\StringTok{"VZ"}\NormalTok{,}\StringTok{"V"}\NormalTok{,}\StringTok{"WMT"}\NormalTok{)}
\NormalTok{e =}\StringTok{ }\KeywordTok{round}\NormalTok{(}\KeywordTok{data.frame}\NormalTok{(e) }\OperatorTok{*}\DecValTok{100}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(e) =}\StringTok{ "Test accuracy(%)"}

\KeywordTok{kable}\NormalTok{(e, }\DataTypeTok{caption =} \StringTok{"Test accuracy, compared"}\NormalTok{, }\DataTypeTok{booktabs =}\NormalTok{ T) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \KeywordTok{c}\NormalTok{(}\StringTok{"hold_position"}\NormalTok{))  }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{latex_options =} \StringTok{"striped"}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{kable_styling}\NormalTok{(}\DataTypeTok{font_size =} \DecValTok{9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\rowcolors{2}{gray!6}{white}

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-7}Test accuracy, compared}
\centering
\fontsize{9}{11}\selectfont
\begin{tabular}[t]{lr}
\hiderowcolors
\toprule
  & Test accuracy(\%)\\
\midrule
\showrowcolors
MMM & 57.8\\
AXP & 52.6\\
AAPL & 30.5\\
BA & 47.4\\
CAT & 63.5\\
\addlinespace
CVX & 47.8\\
CSCO & 37.3\\
KO & 47.8\\
DIS & 40.6\\
XOM & 43.8\\
\addlinespace
GE & 38.6\\
GS & 45.8\\
HD & 46.2\\
IBM & 56.6\\
INTC & 39.8\\
\addlinespace
JNJ & 44.6\\
JPM & 43.8\\
MCD & 33.3\\
MRK & 41.8\\
MSFT & 30.9\\
\addlinespace
NKE & 37.3\\
PFE & 44.2\\
PG & 43.4\\
TRV & 54.2\\
UTX & 58.2\\
\addlinespace
UNH & 41.4\\
VZ & 51.4\\
V & 44.6\\
WMT & 45.0\\
\bottomrule
\end{tabular}
\end{table}

\rowcolors{2}{white}{white}

Sentiment in social media could be the good pedictor to forecast the
market price movement. Due to the improvement of computational power and
analysis techniques it is possible to estimate the uncertain market
behavior better than before. On the other hand, there is still a limit
as there are always uncertain phenomena that is hard to catch. Although
Our result shows that the test accuracy is slightly betther than 50\%,
including other statistical methods to improve the processes that we
used in this report, will enhance the test accuracy.

Another limit from the database we are using is that messages are
already identified as bullish and bearish by data provider. Some of
messages are clear in their sentiment toward financial market while
there are some of them can be considered as quite neutual. In that case,
whether to catigorized them under bullish or bearish can be a very
subjective decision and will produce bias. One way to improve is to
consider add another category of neutral comments, so that we can
collect all neutral and ambiguous comments under this category.

\newpage

\subsection{References}\label{references}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Derek G. Tsui. Predicting Stock Price Movement Using Social Media
  Analysis, Stanford University, 2016
\end{enumerate}


\end{document}
